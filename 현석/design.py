import os
import streamlit as st
from dotenv import load_dotenv

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# ğŸ” API í‚¤ ë¡œë“œ
load_dotenv()
os.environ["OPENAI_API_KEY"] = os.getenv("OPENAI_API_KEY")
print("âœ… Loaded Key:", os.getenv("OPENAI_API_KEY"))

# ğŸŒ Streamlit ì„¤ì •
st.set_page_config(page_title="ğŸ“˜ ê²½ë¶ëŒ€ ì±—ë´‡", layout="centered")

# ğŸ« ìƒë‹¨ ë¡œê³  & íƒ€ì´í‹€
col1, col2 = st.columns([1, 5])
with col1:
    st.image("assets/knu_logo.png", width=70)
with col2:
    st.markdown("<h2 style='margin-top:18px;'>ê²½ë¶ëŒ€í•™êµ AI ë„ìš°ë¯¸</h2>", unsafe_allow_html=True)

# ğŸ“ PDF í´ë” ë‚´ ë¬¸ì„œ í†µí•© ì²˜ë¦¬
PDF_DIR = "./data"
pdf_files = [f for f in os.listdir(PDF_DIR) if f.endswith(".pdf")]

if not pdf_files:
    st.error("âŒ data í´ë”ì— PDFê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

@st.cache_resource
def build_combined_rag(pdf_dir):
    all_docs = []
    for filename in os.listdir(pdf_dir):
        if filename.endswith(".pdf"):
            path = os.path.join(pdf_dir, filename)
            loader = PyPDFLoader(path)
            pages = loader.load_and_split()
            for p in pages:
                p.metadata["source"] = filename
            all_docs.extend(pages)

    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    docs = splitter.split_documents(all_docs)

    # âœ… OpenAI ì„ë² ë”© ì‚¬ìš©
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small", openai_api_key=os.getenv("OPENAI_API_KEY"))
    vectorstore = FAISS.from_documents(docs, embedding=embeddings)
    retriever = vectorstore.as_retriever()

    llm = ChatOpenAI(
        model="gpt-4o",
        temperature=0.3,
        openai_api_key=os.getenv("OPENAI_API_KEY")
    )

    system_prompt = """ë‹¹ì‹ ì€ ê²½ë¶ëŒ€í•™êµ í•™ì‚¬ ê´€ë ¨ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì •ì¤‘í•œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ëŠ” AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤. \
ê°€ì¥ ê´€ë ¨ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ê³ , ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë§í•´ì£¼ì„¸ìš”.\n{context}"""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}")
    ])

    chain = (
        {"context": retriever | (lambda d: "\n\n".join(doc.page_content for doc in d)), "input": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    return chain

# âœ… ì²´ì¸ ì´ˆê¸°í™”
chain = build_combined_rag(PDF_DIR)

# âœ… ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ğŸ“˜ ê²½ë¶ëŒ€ í•™ì‚¬ ê´€ë ¨ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ê¶ê¸ˆí•œ ê±¸ ë¬¼ì–´ë³´ì„¸ìš” :)"}
    ]

# âœ… ëŒ€í™” ë Œë”ë§ (ìƒí™©ì— ë”°ë¥¸ ë§ˆìŠ¤ì½”íŠ¸ ë³€ê²½)
for i, msg in enumerate(st.session_state["messages"]):
    if msg["role"] == "assistant":
        if i == 0:
            mascot_img = "assets/mascot_hello.png"
        elif i == len(st.session_state["messages"]) - 1:
            mascot_img = "assets/mascot_alarm.png"
        else:
            mascot_img = "assets/mascot.png"

        col1, col2 = st.columns([1, 5])
        with col1:
            st.image(mascot_img, width=90)
        with col2:
            st.markdown(f"""
                <div style='background-color:#f0f2f6; padding:15px 20px; 
                            border-radius:15px; margin-bottom:15px;
                            border: 1px solid #ccc; box-shadow: 2px 2px 4px rgba(0,0,0,0.1);'>
                    {msg['content']}
                </div>
            """, unsafe_allow_html=True)

    elif msg["role"] == "user":
        st.markdown(f"<p style='text-align:right; color:#dcdcdc;'>ğŸ™‹â€â™‚ï¸ {msg['content']}</p>", unsafe_allow_html=True)

# âœ… ì‚¬ìš©ì ì…ë ¥
if query := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: íœ´í•™ ì‹ ì²­ì€ ì–´ë–»ê²Œ í•˜ë‚˜ìš”?)"):
    st.session_state["messages"].append({"role": "user", "content": query})
    st.markdown(f"<p style='text-align:right; color:#dcdcdc;'>{query}</p>", unsafe_allow_html=True)

    col1, col2 = st.columns([1, 5])
    with col1:
        st.image("assets/mascot.png", width=90)
    with col2:
        st.markdown("""
            <div style='background-color:#fffbe6; padding:15px 20px; 
                        border-radius:15px; margin-bottom:15px;
                        border: 1px solid #f7d674; box-shadow: 2px 2px 4px rgba(0,0,0,0.1);'>
                ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”. ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ê³  ìˆì–´ìš”... ğŸ•µï¸â€â™‚ï¸
            </div>
        """, unsafe_allow_html=True)

    with st.spinner("ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³  ìˆì–´ìš”..."):
        try:
            answer = chain.invoke(query)
            st.session_state["messages"].append({"role": "assistant", "content": answer})
        except Exception as e:
            st.session_state["messages"].append({
                "role": "assistant",
                "content": f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}\n\nğŸ”‘ OpenAI API í‚¤ê°€ ìœ íš¨í•œì§€ í™•ì¸í•´ ì£¼ì„¸ìš”."
            })
