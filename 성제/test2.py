# ğŸ§  1. ê¸°ë³¸ ëª¨ë“ˆ ë° API í‚¤ ì„¤ì •
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_anthropic import ChatAnthropic
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
import os
import getpass


# ğŸ” Claude API Key ì…ë ¥ ë°›ê¸°
claude_api_key = getpass.getpass("Claude API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
os.environ["ANTHROPIC_API_KEY"] = claude_api_key

# ğŸ“ 2. PDF ë¡œë”© (ê²½ë¶ëŒ€ PDF í´ë” ê²½ë¡œ)
folder_path = r"C:/_vscode/Project_13/ì„±ì œ/ê²½ë¶ëŒ€í•™êµ"
loader = PyPDFDirectoryLoader(folder_path)
documents = loader.load()

# âœ… ë¶ˆëŸ¬ì˜¨ PDF ìˆ˜ í™•ì¸
print(f"ğŸ“„ ë¶ˆëŸ¬ì˜¨ PDF ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ")

# ğŸ“š 3. ì²­í¬ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)
# chunk_size : í•˜ë‚˜ì˜ ì²­í¬ì— ë“¤ì–´ê°ˆ í† í° or ë‹¨ì–´ ìˆ˜ / chunk_overlap : ë‹¤ìŒ í† í°ê³¼ì˜ ì¤‘ì²© í† í° or ë‹¨ì–´ ìˆ˜
# seporators = ["\n\n", "\n", " ", ""]
chunks = text_splitter.split_documents(documents)
print(f"ğŸ§© ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}ê°œ")

# ğŸ“Œ 4. ì„ë² ë”© ëª¨ë¸ (í•œêµ­ì–´ HuggingFace ëª¨ë¸ ì‚¬ìš©)
embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
# model_name ì¶”ì²œ list [BM-K/KoSimCSE-roberta,llm-grounded/ko-sentence-transformers-v1,jhgan/ko-sbert-nli] 

# ğŸ§  5. ë²¡í„° ì €ì¥ì†Œ ìƒì„± (Chroma ì‚¬ìš©)
vectorstore = Chroma.from_documents(documents=chunks, embedding=embedding_model)
retriever = vectorstore.as_retriever()

# ğŸ¤– 6. Claude ê¸°ë°˜ LLM ì„¤ì •
llm = ChatAnthropic(model="claude-3-haiku-20240307")
# model ì¶”ì²œ list ["claude-3-haiku-20240307", "claude-3.5-sonnet-20240620","claude-4-sonnet-20240229"]

# ğŸ”„ 7. RAG ì²´ì¸ êµ¬ì„±
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | RunnableLambda(lambda x: f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n\n{x['context']}\n\nì§ˆë¬¸: {x['question']}")
    | llm
)

# ğŸ§ª 8. í…ŒìŠ¤íŠ¸ ì§ˆë¬¸
query = "ê²½ë¶ëŒ€í•™êµ êµìœ¡ëª©í‘œì— ëŒ€í•´ ì•Œë ¤ì£¼ë¼"
response = rag_chain.invoke(query)
print(f"\nğŸ“¢ ë‹µë³€:\n{response.content}")
