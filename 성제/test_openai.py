import os
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from bert_score import score as bert_score
from getpass import getpass

# âœ… 1. OpenAI API í‚¤ ì„¤ì •
os.environ["OPENAI_API_KEY"] = getpass("ğŸ” OpenAI API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")

# âœ… 2. PDF ê²½ë¡œ ì„¤ì •
pdf_dir = r"C:\_vscode\Project_13\ì„±ì œ\ê²½ë¶ëŒ€í•™êµ"

# âœ… 3. ëª¨ë“  PDF ë¶ˆëŸ¬ì˜¤ê¸°
loaders = [PyPDFLoader(os.path.join(pdf_dir, f))
           for f in os.listdir(pdf_dir) if f.endswith(".pdf")]
print(f"ğŸ“„ ë¶ˆëŸ¬ì˜¨ PDF ìˆ˜: {len(loaders)}")

# âœ… 4. ë¬¸ì„œ ë¡œë“œ ë° ì²­í¬ ë¶„í• 
docs = []
for loader in loaders:
    docs.extend(loader.load())
print(f"ğŸ“ƒ ì „ì²´ ë¬¸ì„œ í˜ì´ì§€ ìˆ˜: {len(docs)}")

splitter = RecursiveCharacterTextSplitter(
    chunk_size=700, chunk_overlap=100,
    separators=["\n\n", "\n", " ", ""]
)
chunks = splitter.split_documents(docs)
print(f"ğŸ§© ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}")

# âœ… 5. ë²¡í„° ì €ì¥ì†Œ ìƒì„±
vectorstore = Chroma.from_documents(
    chunks,
    embedding=OpenAIEmbeddings(model="text-embedding-3-small"),
    persist_directory="./knu_vectorstore"
)
vectorstore.persist()
print("âœ… ë²¡í„° ì €ì¥ì†Œ ìƒì„± ì™„ë£Œ!")

# âœ… 6. ì§ˆì˜ì‘ë‹µ ì²´ì¸ êµ¬ì„±
retriever = vectorstore.as_retriever()
rag_chain = RetrievalQA.from_chain_type(
    llm=ChatOpenAI(model="gpt-3.5-turbo"),
    retriever=retriever
)
gpt_direct = ChatOpenAI(model="gpt-3.5-turbo")

# âœ… 7. í‰ê°€ ë£¨í”„ ì‹œì‘
while True:
    print("\nğŸ’¬ í‰ê°€í•  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œí•˜ë ¤ë©´ 'exit'):")
    query = input("ì§ˆë¬¸: ")
    if query.lower() == "exit":
        break

    reference = input("ğŸ“˜ ëª¨ë²” ë‹µë³€ (Reference)ì„ ì…ë ¥í•˜ì„¸ìš”:\n")

    print("ğŸ¤– RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì¤‘...")
    rag_answer = rag_chain.run(query)

    print("ğŸ¤– ì¼ë°˜ GPT ì‘ë‹µ ìƒì„± ì¤‘...")
    gpt_answer = gpt_direct.predict(query)

    print("\nâœ… [RAG ì‘ë‹µ]:\n", rag_answer)
    print("\nâœ… [GPT ë‹¨ë… ì‘ë‹µ]:\n", gpt_answer)
    print("\nğŸ“˜ [ëª¨ë²” ë‹µë³€]:\n", reference)

    # âœ… BERT-Score ê³„ì‚° (reference ê¸°ì¤€)
    P_rag, R_rag, F1_rag = bert_score(
        [rag_answer], [reference],
        lang="ko",
        model_type="xlm-roberta-large"
    )
    P_gpt, R_gpt, F1_gpt = bert_score(
        [gpt_answer], [reference],
        lang="ko",
        model_type="xlm-roberta-large"
    )

    # âœ… ë¹„êµ ì¶œë ¥
    print("\nğŸ“Š BERT-Score ê²°ê³¼ (ê¸°ì¤€: ëª¨ë²” ë‹µë³€)")
    print("ğŸ§  ì¼ë°˜ GPT ì‘ë‹µ:")
    print(f"  - Precision: {P_gpt.mean():.4f}")
    print(f"  - Recall   : {R_gpt.mean():.4f}")
    print(f"  - F1 Score : {F1_gpt.mean():.4f}")

    print("\nğŸ§  RAG ê¸°ë°˜ GPT ì‘ë‹µ:")
    print(f"  - Precision: {P_rag.mean():.4f}")
    print(f"  - Recall   : {R_rag.mean():.4f}")
    print(f"  - F1 Score : {F1_rag.mean():.4f}")
