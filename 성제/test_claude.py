# ğŸ§  1. ê¸°ë³¸ ëª¨ë“ˆ ë° API í‚¤ ì„¤ì •
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_anthropic import ChatAnthropic
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from bert_score import score as bert_score
import os
import getpass

# ğŸ” Claude API Key ì…ë ¥ ë°›ê¸°
claude_api_key = getpass.getpass("Claude API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
os.environ["ANTHROPIC_API_KEY"] = claude_api_key

# ğŸ“ 2. PDF ë¡œë”©
folder_path = r"C:/_vscode/Project_13/ì„±ì œ/ê²½ë¶ëŒ€í•™êµ"
loader = PyPDFDirectoryLoader(folder_path)
documents = loader.load()
print(f"ğŸ“„ ë¶ˆëŸ¬ì˜¨ PDF ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ")

# ğŸ“š 3. ì²­í¬ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)
chunks = text_splitter.split_documents(documents)
print(f"ğŸ§© ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}ê°œ")

# ğŸ“Œ 4. HuggingFace ì„ë² ë”© ëª¨ë¸
embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")

# ğŸ§  5. ë²¡í„° ì €ì¥ì†Œ ìƒì„±
vectorstore = Chroma.from_documents(documents=chunks, embedding=embedding_model)
retriever = vectorstore.as_retriever()

# ğŸ¤– 6. Claude ëª¨ë¸ ì„¤ì •
llm = ChatAnthropic(model="claude-3-haiku-20240307")

# ğŸ”„ 7. RAG ì²´ì¸ êµ¬ì„±
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | RunnableLambda(lambda x: f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n\n{x['context']}\n\nì§ˆë¬¸: {x['question']}")
    | llm
)

# ğŸ§ª 8. ë¹„êµ ì§ˆë¬¸ ìˆ˜í–‰
query = input("\nğŸ’¬ ë¹„êµí•  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")

# (1) RAG ì‘ë‹µ
print("\nğŸ” RAG ê¸°ë°˜ ì‘ë‹µ ìƒì„± ì¤‘...")
rag_response = rag_chain.invoke(query).content
print(f"\nğŸ“¢ [RAG ì‘ë‹µ]:\n{rag_response}")

# (2) ì¼ë°˜ Claude ì§ì ‘ ì§ˆì˜ ì‘ë‹µ
print("\nğŸ¤– ì¼ë°˜ Claude ì‘ë‹µ ìƒì„± ì¤‘...")
gpt_response = llm.invoke(query).content
print(f"\nğŸ“¢ [ì¼ë°˜ GPT ì‘ë‹µ]:\n{gpt_response}")

# (3) BERT-Score ê³„ì‚°
print("\nğŸ“Š BERT-Score ê³„ì‚° ì¤‘...")
P, R, F1 = bert_score(
    [rag_response],
    [gpt_response],
    lang="ko",  # í•œêµ­ì–´
    model_type="xlm-roberta-large"  # or jhgan/ko-sbert-nli
)

"""
1. ë‘ ë¬¸ì¥ì„ ê°ê° BERT ëª¨ë¸ì„ í†µí•´ í† í° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
2. ë‘ ë¬¸ì¥ì˜ í† í°ìŒ ê°„ cosine similarity ê³„ì‚° (ëª¨ë“  í† í° ì¡°í•©)
3. Precision, Recall, F1 ê³„ì‚°
"""

print("\nâœ… BERT-Score ê²°ê³¼:")
print(f"  - Precision: {P.mean():.4f}")
print(f"  - Recall   : {R.mean():.4f}")
print(f"  - F1 Score : {F1.mean():.4f}")
