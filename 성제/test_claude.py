from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_anthropic import ChatAnthropic
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from bert_score import score as bert_score
import os
import getpass

# ğŸ” Claude API Key ì…ë ¥ ë°›ê¸°
claude_api_key = getpass.getpass("Claude API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")
os.environ["ANTHROPIC_API_KEY"] = claude_api_key

# ğŸ“ PDF ë¡œë”©
folder_path = r"C:/_vscode/Project_13/ì„±ì œ/ê²½ë¶ëŒ€í•™êµ"
loader = PyPDFDirectoryLoader(folder_path)
documents = loader.load()
print(f"ğŸ“„ ë¶ˆëŸ¬ì˜¨ PDF ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ")

# ğŸ“š ì²­í¬ ë¶„í• 
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
chunks = text_splitter.split_documents(documents)
print(f"ğŸ§© ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks)}ê°œ")

# ğŸ¤– ì„ë² ë”© ë° ë²¡í„°ì €ì¥ì†Œ ìƒì„±
embedding_model = HuggingFaceEmbeddings(model_name="jhgan/ko-sroberta-multitask")
vectorstore = Chroma.from_documents(documents=chunks, embedding=embedding_model)
retriever = vectorstore.as_retriever()

# ğŸ§  Claude ëª¨ë¸
llm = ChatAnthropic(model="claude-3-haiku-20240307")

# ğŸ”„ RAG ì²´ì¸
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | RunnableLambda(lambda x: f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n\n{x['context']}\n\nì§ˆë¬¸: {x['question']}")
    | llm
)

# âœ… ì§ˆë¬¸ ë° ì •ë‹µ ì…ë ¥
query = input("\nğŸ’¬ í‰ê°€í•  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
reference_answer = input("ğŸ“˜ í•´ë‹¹ ì§ˆë¬¸ì˜ ëª¨ë²” ë‹µë³€(reference)ì„ ì…ë ¥í•˜ì„¸ìš”: ")

# (1) RAG ì‘ë‹µ
print("\nğŸ” RAG ê¸°ë°˜ Claude ì‘ë‹µ ìƒì„± ì¤‘...")
rag_response = rag_chain.invoke(query).content
print(f"\nğŸ“¢ [RAG ì‘ë‹µ]:\n{rag_response}")

# (2) ì¼ë°˜ Claude ì‘ë‹µ
print("\nğŸ¤– ì¼ë°˜ Claude ì‘ë‹µ ìƒì„± ì¤‘...")
gpt_response = llm.invoke(query).content
print(f"\nğŸ“¢ [ì¼ë°˜ Claude ì‘ë‹µ]:\n{gpt_response}")

# (3) BERT-Score ê³„ì‚°
print("\nğŸ“Š BERT-Score ê³„ì‚° ì¤‘...")

P_rag, R_rag, F1_rag = bert_score(
    [rag_response], [reference_answer], lang="ko", model_type="xlm-roberta-large"
)
P_gpt, R_gpt, F1_gpt = bert_score(
    [gpt_response], [reference_answer], lang="ko", model_type="xlm-roberta-large"
)

# (4) ê²°ê³¼ ì¶œë ¥
print("\nâœ… BERT-Score ê²°ê³¼ (ê¸°ì¤€: ëª¨ë²” ë‹µë³€)")
print("ğŸ§  ì¼ë°˜ Claude ì‘ë‹µ:")
print(f"  - Precision: {P_gpt.mean():.4f}")
print(f"  - Recall   : {R_gpt.mean():.4f}")
print(f"  - F1 Score : {F1_gpt.mean():.4f}")

print("\nğŸ§  RAG ê¸°ë°˜ Claude ì‘ë‹µ:")
print(f"  - Precision: {P_rag.mean():.4f}")
print(f"  - Recall   : {R_rag.mean():.4f}")
print(f"  - F1 Score : {F1_rag.mean():.4f}")
