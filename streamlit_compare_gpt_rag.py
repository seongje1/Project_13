import os
import tempfile
import streamlit as st
from dotenv import load_dotenv

# âœ… .envì—ì„œ OPENAI_API_KEY ë¶ˆëŸ¬ì˜¤ê¸°
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# âœ… Streamlit ì´ˆê¸° ì„¤ì •
st.set_page_config(page_title="ğŸ“˜ GPT-4 vs RAG ì±—ë´‡", layout="wide")
st.title("ğŸ¤– GPT-4 vs ğŸ“„ RAG ì±—ë´‡ ë¹„êµ")
st.write("âœ… ì•±ì´ ì •ìƒì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ğŸ“¤ PDF ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ğŸ“„ ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (PDF)", type=["pdf"])

# ğŸ”§ í•¨ìˆ˜ ì •ì˜
def load_pdf(_file):
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp:
        tmp.write(_file.read())
        tmp_path = tmp.name
    loader = PyPDFLoader(tmp_path)
    return loader.load_and_split()

def create_vectorstore(pages):
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    docs = splitter.split_documents(pages)
    return Chroma.from_documents(docs, OpenAIEmbeddings(model='text-embedding-3-small', openai_api_key=openai_api_key))

def build_rag_chain(_vectorstore):
    retriever = _vectorstore.as_retriever()
    system_prompt = """ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€ë‹µí•˜ëŠ” ì¹œì ˆí•œ ì±—ë´‡ì…ë‹ˆë‹¤. \
ë‹¤ìŒ contextë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ì •ì¤‘í•˜ê³  ì •í™•í•˜ê²Œ ë‹µí•´ì£¼ì„¸ìš”. \
ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”. í•œêµ­ì–´ë¡œ ë‹µí•˜ì„¸ìš”. \
{context}"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])
    llm = ChatOpenAI(model="gpt-4o", temperature=0, openai_api_key=openai_api_key)
    return (
        {"context": retriever | format_docs, "input": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

def gpt4_response(query):
    model = ChatOpenAI(model="gpt-4", temperature=0, openai_api_key=openai_api_key)
    return model.predict(query)

# ğŸ’¬ ì§ˆë¬¸ ì…ë ¥
query = st.text_input("â“ ê²½ë¶ëŒ€í•™êµ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

# âœ… ì‘ë‹µ ë¹„êµ ì¶œë ¥
if uploaded_file and query:
    with st.spinner("PDF ì²˜ë¦¬ ì¤‘..."):
        pages = load_pdf(uploaded_file)
        vectorstore = create_vectorstore(pages)
        rag_chain = build_rag_chain(vectorstore)

    col1, col2 = st.columns(2)
    with col1:
        st.subheader("ğŸŒ GPTâ€‘4 ê¸°ë³¸ ì‘ë‹µ")
        gpt_answer = gpt4_response(query)
        st.write(gpt_answer)

    with col2:
        st.subheader("ğŸ“„ PDF ê¸°ë°˜ RAG ì‘ë‹µ")
        rag_answer = rag_chain.invoke(query)
        st.write(rag_answer)
